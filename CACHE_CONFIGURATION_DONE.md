# Cache Configuration - COMPLETED âœ…

## Summary

All HuggingFace models will now be downloaded to:

```
/ephemeral/shared/spoppi/huggingface/hub/
```

## What Was Done

### 1. Created Centralized Configuration

**File**: `constitutional_guard/config.py`

```python
HF_CACHE_DIR = "/ephemeral/shared/spoppi/huggingface/hub/"
os.environ['HF_HOME'] = HF_CACHE_DIR
os.environ['TRANSFORMERS_CACHE'] = HF_CACHE_DIR
os.environ['HF_DATASETS_CACHE'] = HF_CACHE_DIR
```

### 2. Updated All Scripts

Modified to import cache directory from centralized config:

- âœ… `model/model.py` - ConstitutionalGuard wrapper
- âœ… `training/train.py` - Training pipeline
- âœ… Other scripts import from config.py

### 3. Created Verification Script

**File**: `constitutional_guard/verify_setup.py`

Checks:
- âœ… Cache directory exists and is writable
- âœ… All data files are generated
- âœ… Dependencies are installed (torch, transformers, peft)
- âœ… GPU is available (4x NVIDIA RTX A6000, 47.5 GB each)

### 4. Verified Setup

**Result**:
```
âœ“ Configuration loaded
âœ“ Cache directory: /ephemeral/shared/spoppi/huggingface/hub/
âœ“ Cache directory is writable
âœ“ All data files present (2.7 MB total)
âœ“ All dependencies installed
âœ“ GPU available: NVIDIA RTX A6000 (47.5 GB)
```

## Project Status

### Constitutional Guard - Complete Implementation

**Phase 1: Llama Guard 3 Analysis** âœ…
- Discovered Llama Guard 3 doesn't follow policy semantically
- Artifact control tests showed 77% attention is positional, not semantic
- Behavioral tests confirmed safety alignment dominates
- Flip-rate ~0% (model can't be constrained by policy)

**Phase 2: Constitutional Guard** âœ…
- Complete policy-first architecture designed
- 61 diverse policies generated
- 2731 training examples with flip-pairs
- Flip-consistency loss implemented
- Training pipeline with LoRA ready
- Evaluation suite ready

### Files Created

```
constitutional_guard/
â”œâ”€â”€ config.py                    âœ… NEW - Centralized config
â”œâ”€â”€ verify_setup.py              âœ… NEW - Setup verification
â”œâ”€â”€ SETUP.md                     âœ… NEW - Setup instructions
â”œâ”€â”€ QUICKSTART.md                âœ… Complete
â”œâ”€â”€ PROJECT_STATUS.md            âœ… Complete
â”œâ”€â”€ schema/                      âœ… Complete (2 files)
â”œâ”€â”€ data/                        âœ… Complete (6 files)
â”œâ”€â”€ model/                       âœ… Complete (2 files, updated)
â”œâ”€â”€ training/                    âœ… Complete (1 file, updated)
â””â”€â”€ evaluation/                  âœ… Complete (2 files)
```

## Next Steps for Training

### 1. HuggingFace Login

```bash
huggingface-cli login
```

### 2. Verify Setup

```bash
cd /ephemeral/home/spoppi/projects/random_tests/constitutional_guard
conda run -n const-ai python verify_setup.py
```

### 3. Train Model

```bash
cd /ephemeral/home/spoppi/projects/random_tests/constitutional_guard/training
conda run -n const-ai python train.py
```

**Expected**:
- Model downloads to: `/ephemeral/shared/spoppi/huggingface/hub/`
- Training time: 4-8 hours on RTX A6000
- Output: LoRA checkpoint in `training/constitutional_guard_lora/`

### 4. Evaluate

```bash
cd /ephemeral/home/spoppi/projects/random_tests/constitutional_guard/evaluation
conda run -n const-ai python evaluate.py
```

**Target**: Flip-rate â‰¥95%

## What Models Will Be Downloaded

When you run training, these will be downloaded to cache directory:

1. **meta-llama/Llama-3.1-8B** (~16 GB)
   - Base model for Constitutional Guard
   - Requires HuggingFace access (request at https://huggingface.co/meta-llama/Llama-3.1-8B)

2. **Tokenizer** (~5 MB)
   - Llama 3 tokenizer

3. **PEFT Adapters** (generated during training, ~200 MB)
   - LoRA weights

**Total space needed**: ~17 GB

## Cache Directory Structure

After training, the cache will contain:

```
/ephemeral/shared/spoppi/huggingface/hub/
â”œâ”€â”€ models--meta-llama--Llama-3.1-8B/
â”‚   â”œâ”€â”€ snapshots/
â”‚   â”‚   â””â”€â”€ [hash]/
â”‚   â”‚       â”œâ”€â”€ model-00001-of-00004.safetensors
â”‚   â”‚       â”œâ”€â”€ model-00002-of-00004.safetensors
â”‚   â”‚       â”œâ”€â”€ model-00003-of-00004.safetensors
â”‚   â”‚       â”œâ”€â”€ model-00004-of-00004.safetensors
â”‚   â”‚       â”œâ”€â”€ config.json
â”‚   â”‚       â”œâ”€â”€ tokenizer.json
â”‚   â”‚       â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

## Verification Commands

### Check cache directory

```bash
ls -lh /ephemeral/shared/spoppi/huggingface/hub/
```

### Check disk space

```bash
df -h /ephemeral/shared/spoppi/
```

### Verify configuration is loaded

```bash
cd /ephemeral/home/spoppi/projects/random_tests/constitutional_guard
conda run -n const-ai python -c "from config import HF_CACHE_DIR; print(f'Cache: {HF_CACHE_DIR}')"
```

Expected output:
```
âœ“ Configuration loaded
  HF_CACHE_DIR: /ephemeral/shared/spoppi/huggingface/hub/
  PROJECT_ROOT: /ephemeral/home/spoppi/projects/random_tests/constitutional_guard
Cache: /ephemeral/shared/spoppi/huggingface/hub/
```

## Summary of Changes

| File | Change | Status |
|------|--------|--------|
| config.py | Created centralized config | âœ… NEW |
| model/model.py | Import from config | âœ… UPDATED |
| training/train.py | Import from config | âœ… UPDATED |
| verify_setup.py | Setup verification | âœ… NEW |
| SETUP.md | Setup documentation | âœ… NEW |

## All Systems Ready âœ…

- âœ… Cache directory configured: `/ephemeral/shared/spoppi/huggingface/hub/`
- âœ… Cache directory exists and is writable
- âœ… All scripts updated to use centralized config
- âœ… Setup verification script created and tested
- âœ… Documentation complete
- âœ… Dependencies installed
- âœ… GPU available (4x RTX A6000)
- âœ… Data files generated (2731 training examples)

**Ready for training!** ðŸš€

---

**Date**: 2025-10-27
**Cache Location**: `/ephemeral/shared/spoppi/huggingface/hub/` âœ…
**Status**: Configuration complete, ready for model download and training
